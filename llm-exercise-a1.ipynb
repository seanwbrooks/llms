{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cdb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"STAT 453: Deep Learning (Spring 2021)  \\n\",\n",
    "    \"Instructor: Sebastian Raschka (sraschka@wisc.edu)  \\n\",\n",
    "    \"\\n\",\n",
    "    \"Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2021/  \\n\",\n",
    "    \"GitHub repository: https://github.com/rasbt/stat453-deep-learning-ss21\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Author: Sebastian Raschka\\n\",\n",
    "      \"\\n\",\n",
    "      \"Python implementation: CPython\\n\",\n",
    "      \"Python version       : 3.9.2\\n\",\n",
    "      \"IPython version      : 7.20.0\\n\",\n",
    "      \"\\n\",\n",
    "      \"torch: 1.9.0a0+d819a21\\n\",\n",
    "      \"\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%load_ext watermark\\n\",\n",
    "    \"%watermark -a 'Sebastian Raschka' -v -p torch\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Understanding Onehot Encoding and Cross Entropy in PyTorch\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Onehot Encoding\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"one-hot encoding:\\n\",\n",
    "      \" tensor([[1., 0., 0.],\\n\",\n",
    "      \"        [0., 1., 0.],\\n\",\n",
    "      \"        [0., 0., 1.],\\n\",\n",
    "      \"        [0., 0., 1.]])\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"def to_onehot(y, num_classes):\\n\",\n",
    "    \"    y_onehot = torch.zeros(y.size(0), num_classes)\\n\",\n",
    "    \"    y_onehot.scatter_(1, y.view(-1, 1).long(), 1).float()\\n\",\n",
    "    \"    return y_onehot\\n\",\n",
    "    \"\\n\",\n",
    "    \"y = torch.tensor([0, 1, 2, 2])\\n\",\n",
    "    \"\\n\",\n",
    "    \"y_enc = to_onehot(y, 3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('one-hot encoding:\\\\n', y_enc)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Softmax\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Suppose we have some net inputs Z, where each row is one training example:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"tensor([[-0.3000, -0.5000, -0.5000],\\n\",\n",
    "       \"        [-0.4000, -0.1000, -0.5000],\\n\",\n",
    "       \"        [-0.3000, -0.9400, -0.5000],\\n\",\n",
    "       \"        [-0.9900, -0.8800, -0.5000]])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 4,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"Z = torch.tensor( [[-0.3,  -0.5, -0.5],\\n\",\n",
    "    \"                   [-0.4,  -0.1, -0.5],\\n\",\n",
    "    \"                   [-0.3,  -0.94, -0.5],\\n\",\n",
    "    \"                   [-0.99, -0.88, -0.5]])\\n\",\n",
    "    \"\\n\",\n",
    "    \"Z\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Next, we convert them to \\\"probabilities\\\" via softmax:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$P(y=j \\\\mid z^{(i)}) = \\\\sigma_{\\\\text{softmax}}(z^{(i)}) = \\\\frac{e^{z^{(i)}}}{\\\\sum_{j=0}^{k} e^{z_{k}^{(i)}}}.$$\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"softmax:\\n\",\n",
    "      \" tensor([[0.3792, 0.3104, 0.3104],\\n\",\n",
    "      \"        [0.3072, 0.4147, 0.2780],\\n\",\n",
    "      \"        [0.4263, 0.2248, 0.3490],\\n\",\n",
    "      \"        [0.2668, 0.2978, 0.4354]])\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"def softmax(z):\\n\",\n",
    "    \"    return (torch.exp(z.t()) / torch.sum(torch.exp(z), dim=1)).t()\\n\",\n",
    "    \"\\n\",\n",
    "    \"smax = softmax(Z)\\n\",\n",
    "    \"print('softmax:\\\\n', smax)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The probabilties can then be converted back to class labels based on the largest probability in each row:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 12,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"predicted class labels:  tensor([0, 1, 0, 2])\\n\",\n",
    "      \"true class labels:  tensor([0, 1, 2, 2])\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"def to_classlabel(z):\\n\",\n",
    "    \"    return torch.argmax(z, dim=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('predicted class labels: ', to_classlabel(smax))\\n\",\n",
    "    \"print('true class labels: ', to_classlabel(y_enc))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Cross Entropy\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Next, we compute the cross entropy for each training example:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"$$\\\\mathcal{L}(\\\\mathbf{W}; \\\\mathbf{b}) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} H(T_i, O_i),$$\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"$$H(T_i, O_i) = -\\\\sum_m T_i \\\\cdot log(O_i).$$\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 17,\n",
    "   \"metadata\": {\n",
    "    \"scrolled\": true\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Cross Entropy: tensor([0.9698, 0.8801, 1.0527, 0.8314])\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"def cross_entropy(softmax, y_target):\\n\",\n",
    "    \"    return - torch.sum(torch.log(softmax) * (y_target), dim=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"xent = cross_entropy(smax, y_enc)\\n\",\n",
    "    \"print('Cross Entropy:', xent)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## In PyTorch\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 20,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch.nn.functional as F\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Note that `nll_loss` takes log(softmax) as input:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 23,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"tensor([0.9698, 0.8801, 1.0527, 0.8314])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 23,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"F.nll_loss(torch.log(smax), y, reduction='none')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Note that `cross_entropy` takes logits as input:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 24,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"tensor([0.9698, 0.8801, 1.0527, 0.8314])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 24,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"F.cross_entropy(Z, y, reduction='none')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Defaults\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"By default, nll_loss & cross_entropy are already returning the average over training examples, which is useful for stability during optimization.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 30,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"tensor(0.9335)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 30,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"F.cross_entropy(Z, y)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 31,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"tensor(0.9335)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 31,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"torch.mean(cross_entropy(smax, y_enc))\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.2\"\n",
    "  },\n",
    "  \"toc\": {\n",
    "   \"base_numbering\": 1,\n",
    "   \"nav_menu\": {},\n",
    "   \"number_sections\": true,\n",
    "   \"sideBar\": true,\n",
    "   \"skip_h1_title\": false,\n",
    "   \"title_cell\": \"Table of Contents\",\n",
    "   \"title_sidebar\": \"Contents\",\n",
    "   \"toc_cell\": false,\n",
    "   \"toc_position\": {},\n",
    "   \"toc_section_display\": true,\n",
    "   \"toc_window_display\": false\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247be729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5062bc53",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
